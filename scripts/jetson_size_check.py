#!/usr/bin/env python3
"""
Analyze Gemma 3n model size and estimate Jetson Nano compatibility
"""

import os
import json
from pathlib import Path

def analyze_model_size():
    """Analyze the current model size and estimate quantization results"""
    
    model_path = "./models/gemma3n-local-e2b"
    
    if not os.path.exists(model_path):
        print("‚ùå Model not found. Please run download_gemma_local.py first.")
        return
    
    print("üìä Analyzing Gemma 3n E2B model size...")
    
    # Get current model size
    total_size = 0
    file_sizes = {}
    
    for root, dirs, files in os.walk(model_path):
        for file in files:
            file_path = os.path.join(root, file)
            file_size = os.path.getsize(file_path)
            total_size += file_size
            file_sizes[file] = file_size
    
    # Calculate size breakdown
    safetensors_size = sum(size for file, size in file_sizes.items() if file.endswith('.safetensors'))
    config_size = sum(size for file, size in file_sizes.items() if file.endswith('.json'))
    other_size = total_size - safetensors_size - config_size
    
    print(f"\nüìÅ Current Model Size Breakdown:")
    print(f"   Total: {format_size(total_size)}")
    print(f"   Model weights (.safetensors): {format_size(safetensors_size)}")
    print(f"   Configuration files: {format_size(config_size)}")
    print(f"   Other files: {format_size(other_size)}")
    
    # Estimate quantization results
    print(f"\nüîÆ Quantization Estimates:")
    
    # 4-bit quantization reduces weights by ~75%
    quantized_weights = safetensors_size * 0.25  # 75% reduction
    quantized_total = quantized_weights + config_size + other_size
    
    print(f"   Quantized weights: {format_size(quantized_weights)}")
    print(f"   Quantized total: {format_size(quantized_total)}")
    print(f"   Size reduction: {((total_size - quantized_total) / total_size * 100):.1f}%")
    
    # Memory usage estimates
    print(f"\nüíæ Memory Usage Estimates:")
    print(f"   Current model RAM: ~{format_size(total_size * 0.9)}")
    print(f"   Quantized model RAM: ~{format_size(quantized_total * 0.9)}")
    
    # Jetson Nano compatibility
    jetson_ram = 4 * 1024 * 1024 * 1024  # 4GB in bytes
    jetson_storage = 16 * 1024 * 1024 * 1024  # 16GB in bytes
    
    print(f"\nüöÄ Jetson Nano Compatibility:")
    print(f"   Jetson Nano RAM: {format_size(jetson_ram)}")
    print(f"   Jetson Nano Storage: {format_size(jetson_storage)}")
    
    ram_compatible = quantized_total * 0.9 < jetson_ram * 0.8  # Leave 20% buffer
    storage_compatible = quantized_total < jetson_storage * 0.8  # Leave 20% buffer
    
    print(f"\n‚úÖ Compatibility Results:")
    print(f"   RAM compatible: {'‚úÖ YES' if ram_compatible else '‚ùå NO'}")
    print(f"   Storage compatible: {'‚úÖ YES' if storage_compatible else '‚ùå NO'}")
    
    if ram_compatible and storage_compatible:
        print(f"\nüéâ The quantized Gemma 3n E2B model WILL fit on Jetson Nano!")
        print(f"   Expected RAM usage: {format_size(quantized_total * 0.9)}")
        print(f"   Expected storage: {format_size(quantized_total)}")
    else:
        print(f"\n‚ö†Ô∏è The model may not fit on Jetson Nano")
        if not ram_compatible:
            print(f"   RAM usage too high: {format_size(quantized_total * 0.9)} > {format_size(jetson_ram * 0.8)}")
        if not storage_compatible:
            print(f"   Storage too large: {format_size(quantized_total)} > {format_size(jetson_storage * 0.8)}")
    
    # Performance estimates
    print(f"\n‚ö° Performance Estimates for Jetson Nano:")
    print(f"   Model loading time: 30-60 seconds")
    print(f"   Audio transcription: 2-5 seconds per token")
    print(f"   30-second audio file: 30-60 seconds total")
    print(f"   Quality degradation: ~1-2% (minimal)")
    
    return {
        'current_size': total_size,
        'quantized_size': quantized_total,
        'ram_compatible': ram_compatible,
        'storage_compatible': storage_compatible
    }

def format_size(size_bytes):
    """Format bytes to human readable format"""
    if size_bytes == 0:
        return "0B"
    size_names = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.1f}{size_names[i]}"

def create_jetson_runner():
    """Create a Jetson Nano optimized runner script"""
    
    runner_code = '''#!/usr/bin/env python3
"""
Jetson Nano optimized Gemma 3n runner
"""

import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
import gc
import time
import os

class JetsonGemmaRunner:
    def __init__(self, model_path="./models/gemma3n-local-e2b"):
        self.model_path = model_path
        self.processor = None
        self.model = None
        
    def load_model(self):
        """Load model with Jetson Nano optimizations"""
        print("üîÑ Loading model for Jetson Nano...")
        
        # Force CPU usage and memory optimizations
        torch.set_num_threads(4)  # Use all 4 cores
        
        self.processor = AutoProcessor.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        # Load with memory optimizations
        self.model = AutoModelForImageTextToText.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        print("‚úÖ Model loaded successfully on Jetson Nano")
        
    def transcribe_audio(self, audio_file_path):
        """Transcribe audio with memory management"""
        if not self.model:
            self.load_model()
            
        try:
            # Load and process audio
            with open(audio_file_path, "rb") as f:
                audio_data = f.read()
            
            # Process with memory cleanup
            inputs = self.processor(
                audio_data,
                return_tensors="pt",
                sampling_rate=16000
            )
            
            # Generate with memory management
            with torch.no_grad():
                start_time = time.time()
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.1,
                    top_p=0.9,
                    pad_token_id=self.processor.tokenizer.pad_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id
                )
                generation_time = time.time() - start_time
            
            # Decode and clean output
            transcription = self.processor.batch_decode(
                outputs, 
                skip_special_tokens=True
            )[0]
            
            # Clean up memory
            del inputs, outputs
            gc.collect()
            
            print(f"‚è±Ô∏è Generation time: {generation_time:.2f}s")
            return transcription.strip()
            
        except Exception as e:
            print(f"‚ùå Error during transcription: {e}")
            return None

if __name__ == "__main__":
    runner = JetsonGemmaRunner()
    print("üöÄ Jetson Nano Gemma 3n runner ready!")
    print("üí° Use: runner.transcribe_audio('your_audio.wav')")
'''
    
    with open("jetson_gemma_runner.py", "w") as f:
        f.write(runner_code)
    
    print("‚úÖ Created jetson_gemma_runner.py for Jetson Nano usage")

if __name__ == "__main__":
    print("üöÄ Analyzing Gemma 3n for Jetson Nano compatibility...")
    results = analyze_model_size()
    print("\nüìù Creating Jetson Nano runner...")
    create_jetson_runner()
    
    if results['ram_compatible'] and results['storage_compatible']:
        print("\nüéâ RECOMMENDATION: Jetson Nano is suitable for this model!")
        print("   You can proceed with ordering the Jetson Nano.")
    else:
        print("\n‚ö†Ô∏è RECOMMENDATION: Consider alternatives:")
        print("   - Jetson Xavier NX (8GB RAM)")
        print("   - Cloud deployment")
        print("   - Desktop with more RAM") 